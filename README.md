# CV Project
<!-- vscode-markdown-toc -->
* 1. [Videoavatars](#Videoavatars)
	* 1.1. [Step1](#Step1)
		* 1.1.1. [Get keypoints.hdf5](#Getkeypoints.hdf5)
		* 1.1.2. [Get masks.hdf5](#Getmasks.hdf5)
		* 1.1.3. [Get camera.pkl](#Getcamera.pkl)
	* 1.2. [Step2](#Step2)
	* 1.3. [Step3](#Step3)
* 2. [HMR2](#HMR2)
* 3. [SMPL](#SMPL)
* 4. [VIBE](#VIBE)
	* 4.1. [Base](#Base)
	* 4.2. [Smooth](#Smooth)
* 5. [BEDLAM](#BEDLAM)
* 6. [humannerf](#humannerf)
    * 6.1. [train](#train)
	* 6.2. [running](#running)

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->

##  1. <a name='Videoavatars'></a>Videoavatars

Do this in ubuntu.

###  1.1. <a name='Step1'></a>Step1 

Need `keypoints.hdf5 masks.hdf5 camera.pkl`.

####  1.1.1. <a name='Getkeypoints.hdf5'></a>Get keypoints.hdf5

Do this in windows.

Use `openpose`, download the binary version of `openpose`. In `openpose_bin` folder, use
```
.\bin\OpenPoseDemo.exe --video video_path --write_json output_json_folder
```
to get the json files contain keypoints. But the keypoints are generated by `body25` model, so we should rewrite the json files to get `coco` model output.

For `coco`
```c++
    {0,  "Nose"},
    {1,  "Neck"},
    {2,  "RShoulder"},
    {3,  "RElbow"},
    {4,  "RWrist"},
    {5,  "LShoulder"},
    {6,  "LElbow"},
    {7,  "LWrist"},
    {8,  "RHip"},
    {9,  "RKnee"},
    {10, "RAnkle"},
    {11, "LHip"},
    {12, "LKnee"},
    {13, "LAnkle"},
    {14, "REye"},
    {15, "LEye"},
    {16, "REar"},
    {17, "LEar"}
```
For `body25`
```c++
    {0,  "Nose"},
    {1,  "Neck"},
    {2,  "RShoulder"},
    {3,  "RElbow"},
    {4,  "RWrist"},
    {5,  "LShoulder"},
    {6,  "LElbow"},
    {7,  "LWrist"},
    {8,  "MidHip"},
    {9,  "RHip"},
    {10, "RKnee"},
    {11, "RAnkle"},
    {12, "LHip"},
    {13, "LKnee"},
    {14, "LAnkle"},
    {15, "REye"},
    {16, "LEye"},
    {17, "REar"},
    {18, "LEar"},
    {19, "LBigToe"},
    {20, "LSmallToe"},
    {21, "LHeel"},
    {22, "RBigToe"},
    {23, "RSmallToe"},
    {24, "RHeel"}
```
We get `body25` have `8, 19, 20, 21, 22, 23, 24` different from `coco`, so we can delete these points. Use `rewrite_json.py`. Then we get the right jsons.

Then we use `2djoints2hdf5.py`, which can run in windows, to get `keypoints.hdf5`.

####  1.1.2. <a name='Getmasks.hdf5'></a>Get masks.hdf5

First, we should use `video2masks.py` to get the masks of this video. Then use `masks2hdf5.py` to get `masks.hdf5`.

####  1.1.3. <a name='Getcamera.pkl'></a>Get camera.pkl

First, we should use `camera_calibration.py` to calibrate our camera, this need a list of images , we put these in `chessboard`. And we put them in the same folder `camera_calibration`. We get $f_x$ and $f_y$, then use them in `create_camera.py`.

Above all, we have got `keypoints.hdf5 masks.hdf5 camera.pkl`. Then change to ubuntu, run `run_step1.sh`. This step we get `reconstructed_poses.hdf5`.

###  1.2. <a name='Step2'></a>Step2

In ubuntu, run `run_step2.sh`. This step we get `consensus.pkl` which will be used to drive SMPL and `consensus.obj` which is the 3D model we construct.

###  1.3. <a name='Step3'></a>Step3

In ubuntu, run `run_step3.sh`. This step we get the texture which is a jpg file.

We got the 3D model and the corresponding textures now. Then we should find how to get pose.

##  2. <a name='HMR2'></a>HMR2

It is different to `hmr`, because `hmr2` can run in `py3.7` environment.

First, we use `video2frames.py` get frames of the video.

Then clone [`hmr2.0`](https://github.com/russoale/hmr2.0), and modify `./hmr2.0/src/visualise/demo.py`. For every frame, we need run `demo.py`, so we can modify it to run only once, delete
```python
parser.add_argument('--image', required=False, default='coco1.png')
```
this line. And change this line into 

```python
    input_folder_path = '../../../dance_frames/'
    for filename in tqdm(sorted(os.listdir(input_folder_path))):
        if filename.endswith(".png"):
            original_img, input_img, params = preprocess_image(os.path.join(input_folder_path, filename), config.ENCODER_INPUT_SHAPE[0])
            result = model.detect(input_img)
            pose = np.squeeze(result['pose'].numpy())
            pose = rotmat2rotvec(pose)

            np.save(f'../../../dance_poses/pose_{pose_num:04d}.npy', pose)
            pose_num = pose_num + 1
```

Then we get 24x3x3 rotation matrix that `hmr` detect, but SMPL need 24x3 angle params. Use `rotmat2rotvec()` to convert rotation matrix to rotation vector.

```python
from scipy.spatial.transform import Rotation

def rotmat2rotvec(pose):
    num_joints = pose.shape[0]
    axis_angle_params = np.zeros((num_joints, 3))

    for i in range(num_joints):
        rotation_matrix = pose[i]
        rotation = Rotation.from_matrix(rotation_matrix)
        axis_angle_params[i] = rotation.as_rotvec()
    
    return axis_angle_params
```

And we need not to visualize the result, so delete the code below. Then we can get dance poses in npy format by run `python demo.py`.

```python
    cam = np.squeeze(result['cam'].numpy())[:3]
    vertices = np.squeeze(result['vertices'].numpy())
    joints = np.squeeze(result['kp2d'].numpy())
    joints = ((joints + 1) * 0.5) * params['img_size']

    renderer = TrimeshRenderer()
    visualize(renderer, original_img, params, vertices, cam, joints)
```

##  3. <a name='SMPL'></a>SMPL

We get 3D model and poses above, so in ubuntu environment, we can drive SMPL to get 3D model in different poses. 

First, we download [`SMPL`](http://smpl.is.tue.mpg.de/), maybe it has been done in `videoavatars`. Then we need the model in `./models/` and `lbs.py posemapper.py serialization.py verts.py` in `smpl_webuser`. And we need `smpl.py` in `./videoavatars/models/` and `basicModel_ft.npy basicModel_vt.npy` in `./videoavatars/asserts/`. Then we write `drive_SMPL.py`.

We put above all in `drive_SMPL` folder. This step we get models in different poses. Next create the video.

This is done in windows. We create model2video folder, put models in `./data/models` and texture in `./data/texture`. Then run `python model2video.py`. We get the video in `./output`.

##  4. <a name='VIBE'></a>VIBE

It needs gpu. So we run it on the server.

###  4.1. <a name='Base'></a>Base

First, clone [`VIBE`](https://github.com/mkocabas/VIBE)

Also we only have to get pose. So there is no need to render or display. And we can't use `yolo` model to run `VIBE`, so we use `maskrcnn`. We can run
```
python demo.py --detector maskrcnn --vid_file YourVideoName.mp4 --output_folder output/ --no_render
```

to get `vibe_output.pkl` in `./output/YourVideoName/`.

Then we can get pose data in this pkl file. We can run `vibe2pose.py` to get poses.

###  4.2. <a name='Smooth'></a>Smooth

We know that VIBE have provided smooth way.

```python
    parser.add_argument('--smooth', action='store_true',
                        help='smooth the results to prevent jitter')

    parser.add_argument('--smooth_min_cutoff', type=float, default=0.004,
                        help='one euro filter min cutoff. '
                             'Decreasing the minimum cutoff frequency decreases slow speed jitter')

    parser.add_argument('--smooth_beta', type=float, default=0.7,
                        help='one euro filter beta. '
                             'Increasing the speed coefficient(beta) decreases speed lag.')
```

So we can run
```
python demo.py --detector maskrcnn --vid_file YourVideoName.mp4 --output_folder output/ --no_render --smooth
```

Then we can get pose data in this pkl file in the same way above.

##  5. <a name='BEDLAM'></a>BEDLAM

First, clone [`BEDLAM`](https://github.com/pixelite1201/BEDLAM).

As we only have to get poses, in `./train/core/tester.py`, we should comment the following code.

```python
focal_length = (img_w * img_w + img_h * img_h) ** 0.5
pred_vertices_array = (hmr_output['vertices'] + hmr_output['pred_cam_t'].unsqueeze(1)).detach().cpu().numpy()
renderer = Renderer(focal_length=focal_length[0], img_w=img_w[0], img_h=img_h[0],
                    faces=self.smplx_cam_head.smplx.faces,
                    same_mesh_color=False)
front_view = renderer.render_front_view(pred_vertices_array,
                                        bg_img_rgb=img.copy())

# save rendering results
basename = img_fname.split('/')[-1]
filename = basename + "pred_%s.jpg" % 'bedlam'
filename_orig = basename + "orig_%s.jpg" % 'bedlam'
front_view_path = os.path.join(output_folder, filename)
orig_path = os.path.join(output_folder, filename_orig)
logger.info(f'Writing output files to {output_folder}')
cv2.imwrite(front_view_path, front_view[:, :, ::-1])
cv2.imwrite(orig_path, img[:, :, ::-1])
renderer.delete()
```
And add the following code. Because the pose matrix we get is 22x3x3, we add two identity matrix. And change the rotation matrix to rotation vector and save them.

```python
hmr_output = self.model(inp_images, bbox_center=bbox_center, bbox_scale=bbox_scale, img_w=img_w, img_h=img_h)
                
pose = np.array(hmr_output['pred_pose'].detach().cpu().numpy().tolist())[0]

matrix = np.eye(3)
pose = np.concatenate((pose, [matrix], [matrix]), axis=0)
num_joints = pose.shape[0]
axis_angle_params = np.zeros((num_joints, 3))

for i in range(num_joints):
    # Extract the rotation matrix for the i-th joint
    rotation_matrix = pose[i]

    # Convert the rotation matrix to axis-angle representation
    rotation = Rotation.from_matrix(rotation_matrix)
    axis_angle_params[i] = rotation.as_rotvec()
# print(axis_angle_params.shape)
np.save(f'./poses/{self.num:04d}', axis_angle_params)
self.num += 1
```

##  6. <a name='humannerf'></a>humannerf

###  6.1. <a name='train'></a>train

In this part we need to get the frames and masks of the video as PNG file, which has been down in the previous steps. What's more, a json file contains metadata for video frames, including human body pose and camera pose, is needed. We can obtain it by running VIBE. Just like in 4.1, we run ```get_json.py``` in `./output/YourVideoName/` to get ```metadata.json```.

Then we put the three files in ```dataset/wild/monocular``` with the following strusture:
```
monocular
    ├── images
    │   └── ${item_id}.png
    ├── masks
    │   └── ${item_id}.png
    └── metadata.json
```
Then run
```shell
python prepare_dataset.py --cfg wild.yaml
```
in ```tools/prepare_wild```

Now we can train the model. It needs 4 gpus. You can change how many and which gpu you want to use in ```configs/config.py```
```python
cfg.n_gpus = 4
    if cfg.n_gpus > 0:
        all_gpus = list(range(cfg.n_gpus))
        cfg.primary_gpus = [0]
        if cfg.n_gpus > 1:
            cfg.secondary_gpus = [g for g in all_gpus \
                                    if g not in cfg.primary_gpus]
        else:
            cfg.secondary_gpus = cfg.primary_gpus
```
And then run
```
python train.py --cfg configs/human_nerf/wild/monocular/adventure.yaml
```
This training process will take around 70h in 225 server.

###  6.2. <a name='running'></a>running
After the training process you get ```latest.tar``` in ```humannerf/experiments/human_nerf/wild/monocular/adventure```. Now we need to user our own pose to drive it. Since the code structure is too complex for us to create a brand new rendering type, we modify the tpose to get what we want. Change ```humannerf/core/data/human_nerf/tpose.py``` in the following way

Change the  ```__init__``` line 50
```python
pose_path = '../poses'
self.total_frames = len(os.listdir(pose_path))
```
This makes the number to be rendered as the total frames in our video

Then change ```__getitem__``` line 142
```python
dst_poses = np.load(f'../poses/{idx:04d}.npy').reshape(72).astype('float32')
```
This load the poses of each frame. Don't forget to rotate the pose to get the correct perspective.
```python
angle = np.pi
add_rmtx = cv2.Rodrigues(np.array([0, 0, -angle], dtype='float32'))[0]
root_rmtx = cv2.Rodrigues(dst_poses[:3])[0]
new_root_rmtx = add_rmtx@root_rmtx
dst_poses[:3] = cv2.Rodrigues(new_root_rmtx)[0][:, 0]
add_rmtx = cv2.Rodrigues(np.array([0, -angle, 0], dtype='float32'))[0]
root_rmtx = cv2.Rodrigues(dst_poses[:3])[0]
new_root_rmtx = add_rmtx@root_rmtx
dst_poses[:3] = cv2.Rodrigues(new_root_rmtx)[0][:, 0]
```

Then run 
```
python run.py \
    --type tpose \
    --cfg configs/human_nerf/wild/monocular/adventure.yaml 
```
And you will get all the frames in 
```humannerf/experiments/human_nerf/wild/monocular/adventure/latest/tpose```

Sythesis all the frames to get the video.